{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbea52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import regex as re\n",
    "import heapq\n",
    "import random\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5d7724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = \"/root/workspace/cs336/assignment1/tests/fixtures/corpus.en\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "merge_file_path = \"/root/workspace/cs336/assignment1/mergeslist.txt\"\n",
    "merge_ops = 244\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "num_processes = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f5d26335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def pre_tokenize(filepath, bound_st, bound_ed, pattern, special_tokens):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        f.seek(bound_st)\n",
    "        chunk = f.read(bound_ed - bound_st).decode(\"utf-8\", errors=\"ignore\")\n",
    "        special_pat = \"|\".join(map(re.escape, special_tokens))\n",
    "        chunk_set = [s for s in re.split(special_pat, chunk) if s]\n",
    "        corpus_weights = {}\n",
    "        for small_chunk in chunk_set:\n",
    "            splited_text = re.findall(pattern, small_chunk)\n",
    "            for words in splited_text:\n",
    "                data_u8 = words.encode(\"utf-8\")\n",
    "                corpus_weights[data_u8] = corpus_weights.get(data_u8, 0) + 1\n",
    "    return corpus_weights\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ba583e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_text_path, \"rb\") as f:\n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c05ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parellel_params = [(train_text_path, start, end, PAT, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "12a06e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProcessPoolExecutor(max_workers=num_processes) as ex:\n",
    "    results = list(ex.map(pre_tokenize, *zip(*parellel_params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4054022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_weights = {}   #{words: (word_now,frequency)}\n",
    "dict_of_pair = {}   #{(ch1, ch2): frequency}, true frequency\n",
    "pair_to_words = {}  #{(ch1, ch2): set(words)}\n",
    "tokens = {i:(i,) for i in range(256)}  #{token_id: [bytestring]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e83ff0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化 word_weights, dict_of_pair, pair_to_words\n",
    "for dic in results:\n",
    "    for k, v in dic.items():\n",
    "        word_weights[k] = word_weights.get(k, (k,0))\n",
    "        word_weights[k] = (k, word_weights[k][1] + v)\n",
    "for k,v in word_weights.items():\n",
    "    for i in range(len(k)-1):\n",
    "        ch1 = k[i]\n",
    "        ch2 = k[i+1]\n",
    "        pair = (ch1, ch2)\n",
    "        dict_of_pair[pair] = dict_of_pair.get(pair, 0) + v[1]\n",
    "        pair_to_words.setdefault(pair, set()).add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba1e9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化优先队列\n",
    "pair_freq_heap = [(-freq, pair) for pair, freq in dict_of_pair.items()]\n",
    "heapq.heapify(pair_freq_heap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b25410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-63482199, (32, 116))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8a9af389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 499 , pair (97, 98)))\r"
     ]
    }
   ],
   "source": [
    "valid_merge = 0\n",
    "token_id = 256\n",
    "while(valid_merge < merge_ops):\n",
    "    neg_freq, pair = heapq.heappop(pair_freq_heap)\n",
    "    freq = -neg_freq\n",
    "    if dict_of_pair.get(pair, 0) != freq:\n",
    "        continue\n",
    "    \n",
    "    idx_now = token_id\n",
    "    token_id += 1\n",
    "    tokens[idx_now] = pair\n",
    "    \n",
    "    for word_id in pair_to_words[pair]:\n",
    "        new_word = []\n",
    "        word = word_weights[word_id][0]\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i + 1 < len(word) and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                new_word.append(idx_now)\n",
    "                i += 2           \n",
    "                dict_of_pair[pair] -= word_weights[word_id][1]\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        \n",
    "        for i in range(len(new_word)-1):\n",
    "            if new_word[i] == idx_now:\n",
    "                if i + 1 < len(new_word):\n",
    "                    if new_word[i+1] == idx_now:\n",
    "                        new_pair_post = (idx_now, idx_now)\n",
    "                        old_pair_post = (pair[1],pair[0])\n",
    "                        dict_of_pair[new_pair_post] = dict_of_pair.get(new_pair_post, 0) + word_weights[word_id][1]\n",
    "                        dict_of_pair[old_pair_post] = dict_of_pair.get(old_pair_post, 0) - word_weights[word_id][1]\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[new_pair_post], new_pair_post))\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[old_pair_post], old_pair_post))   \n",
    "                        pair_to_words.setdefault(new_pair_post, set()).add(word_id)\n",
    "                    else:\n",
    "                        new_pair_post = (idx_now, new_word[i+1])\n",
    "                        old_pair_post = (pair[1], new_word[i+1])\n",
    "                        dict_of_pair[new_pair_post] = dict_of_pair.get(new_pair_post, 0) + word_weights[word_id][1]\n",
    "                        dict_of_pair[old_pair_post] = dict_of_pair.get(old_pair_post, 0) - word_weights[word_id][1]\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[new_pair_post], new_pair_post))\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[old_pair_post], old_pair_post))\n",
    "                        pair_to_words.setdefault(new_pair_post, set()).add(word_id)\n",
    "                if i > 0:\n",
    "                    if new_word[i-1] == idx_now:\n",
    "                        pass\n",
    "                    else:\n",
    "                        new_pair_pre = (new_word[i-1], idx_now)\n",
    "                        old_pair_pre = (new_word[i-1], pair[0])\n",
    "                        dict_of_pair[new_pair_pre] = dict_of_pair.get(new_pair_pre, 0) + word_weights[word_id][1]\n",
    "                        dict_of_pair[old_pair_pre] = dict_of_pair.get(old_pair_pre, 0) - word_weights[word_id][1]\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[new_pair_pre], new_pair_pre))\n",
    "                        heapq.heappush(pair_freq_heap, (-dict_of_pair[old_pair_pre], old_pair_pre))\n",
    "                        pair_to_words.setdefault(new_pair_pre, set()).add(word_id)\n",
    "\n",
    "        word_weights[word_id] = (new_word, word_weights[word_id][1])\n",
    "        \n",
    "    valid_merge += 1\n",
    "    print(f\"Token {idx_now} , pair {pair}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "55623095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0,),\n",
       " 1: (1,),\n",
       " 2: (2,),\n",
       " 3: (3,),\n",
       " 4: (4,),\n",
       " 5: (5,),\n",
       " 6: (6,),\n",
       " 7: (7,),\n",
       " 8: (8,),\n",
       " 9: (9,),\n",
       " 10: (10,),\n",
       " 11: (11,),\n",
       " 12: (12,),\n",
       " 13: (13,),\n",
       " 14: (14,),\n",
       " 15: (15,),\n",
       " 16: (16,),\n",
       " 17: (17,),\n",
       " 18: (18,),\n",
       " 19: (19,),\n",
       " 20: (20,),\n",
       " 21: (21,),\n",
       " 22: (22,),\n",
       " 23: (23,),\n",
       " 24: (24,),\n",
       " 25: (25,),\n",
       " 26: (26,),\n",
       " 27: (27,),\n",
       " 28: (28,),\n",
       " 29: (29,),\n",
       " 30: (30,),\n",
       " 31: (31,),\n",
       " 32: (32,),\n",
       " 33: (33,),\n",
       " 34: (34,),\n",
       " 35: (35,),\n",
       " 36: (36,),\n",
       " 37: (37,),\n",
       " 38: (38,),\n",
       " 39: (39,),\n",
       " 40: (40,),\n",
       " 41: (41,),\n",
       " 42: (42,),\n",
       " 43: (43,),\n",
       " 44: (44,),\n",
       " 45: (45,),\n",
       " 46: (46,),\n",
       " 47: (47,),\n",
       " 48: (48,),\n",
       " 49: (49,),\n",
       " 50: (50,),\n",
       " 51: (51,),\n",
       " 52: (52,),\n",
       " 53: (53,),\n",
       " 54: (54,),\n",
       " 55: (55,),\n",
       " 56: (56,),\n",
       " 57: (57,),\n",
       " 58: (58,),\n",
       " 59: (59,),\n",
       " 60: (60,),\n",
       " 61: (61,),\n",
       " 62: (62,),\n",
       " 63: (63,),\n",
       " 64: (64,),\n",
       " 65: (65,),\n",
       " 66: (66,),\n",
       " 67: (67,),\n",
       " 68: (68,),\n",
       " 69: (69,),\n",
       " 70: (70,),\n",
       " 71: (71,),\n",
       " 72: (72,),\n",
       " 73: (73,),\n",
       " 74: (74,),\n",
       " 75: (75,),\n",
       " 76: (76,),\n",
       " 77: (77,),\n",
       " 78: (78,),\n",
       " 79: (79,),\n",
       " 80: (80,),\n",
       " 81: (81,),\n",
       " 82: (82,),\n",
       " 83: (83,),\n",
       " 84: (84,),\n",
       " 85: (85,),\n",
       " 86: (86,),\n",
       " 87: (87,),\n",
       " 88: (88,),\n",
       " 89: (89,),\n",
       " 90: (90,),\n",
       " 91: (91,),\n",
       " 92: (92,),\n",
       " 93: (93,),\n",
       " 94: (94,),\n",
       " 95: (95,),\n",
       " 96: (96,),\n",
       " 97: (97,),\n",
       " 98: (98,),\n",
       " 99: (99,),\n",
       " 100: (100,),\n",
       " 101: (101,),\n",
       " 102: (102,),\n",
       " 103: (103,),\n",
       " 104: (104,),\n",
       " 105: (105,),\n",
       " 106: (106,),\n",
       " 107: (107,),\n",
       " 108: (108,),\n",
       " 109: (109,),\n",
       " 110: (110,),\n",
       " 111: (111,),\n",
       " 112: (112,),\n",
       " 113: (113,),\n",
       " 114: (114,),\n",
       " 115: (115,),\n",
       " 116: (116,),\n",
       " 117: (117,),\n",
       " 118: (118,),\n",
       " 119: (119,),\n",
       " 120: (120,),\n",
       " 121: (121,),\n",
       " 122: (122,),\n",
       " 123: (123,),\n",
       " 124: (124,),\n",
       " 125: (125,),\n",
       " 126: (126,),\n",
       " 127: (127,),\n",
       " 128: (128,),\n",
       " 129: (129,),\n",
       " 130: (130,),\n",
       " 131: (131,),\n",
       " 132: (132,),\n",
       " 133: (133,),\n",
       " 134: (134,),\n",
       " 135: (135,),\n",
       " 136: (136,),\n",
       " 137: (137,),\n",
       " 138: (138,),\n",
       " 139: (139,),\n",
       " 140: (140,),\n",
       " 141: (141,),\n",
       " 142: (142,),\n",
       " 143: (143,),\n",
       " 144: (144,),\n",
       " 145: (145,),\n",
       " 146: (146,),\n",
       " 147: (147,),\n",
       " 148: (148,),\n",
       " 149: (149,),\n",
       " 150: (150,),\n",
       " 151: (151,),\n",
       " 152: (152,),\n",
       " 153: (153,),\n",
       " 154: (154,),\n",
       " 155: (155,),\n",
       " 156: (156,),\n",
       " 157: (157,),\n",
       " 158: (158,),\n",
       " 159: (159,),\n",
       " 160: (160,),\n",
       " 161: (161,),\n",
       " 162: (162,),\n",
       " 163: (163,),\n",
       " 164: (164,),\n",
       " 165: (165,),\n",
       " 166: (166,),\n",
       " 167: (167,),\n",
       " 168: (168,),\n",
       " 169: (169,),\n",
       " 170: (170,),\n",
       " 171: (171,),\n",
       " 172: (172,),\n",
       " 173: (173,),\n",
       " 174: (174,),\n",
       " 175: (175,),\n",
       " 176: (176,),\n",
       " 177: (177,),\n",
       " 178: (178,),\n",
       " 179: (179,),\n",
       " 180: (180,),\n",
       " 181: (181,),\n",
       " 182: (182,),\n",
       " 183: (183,),\n",
       " 184: (184,),\n",
       " 185: (185,),\n",
       " 186: (186,),\n",
       " 187: (187,),\n",
       " 188: (188,),\n",
       " 189: (189,),\n",
       " 190: (190,),\n",
       " 191: (191,),\n",
       " 192: (192,),\n",
       " 193: (193,),\n",
       " 194: (194,),\n",
       " 195: (195,),\n",
       " 196: (196,),\n",
       " 197: (197,),\n",
       " 198: (198,),\n",
       " 199: (199,),\n",
       " 200: (200,),\n",
       " 201: (201,),\n",
       " 202: (202,),\n",
       " 203: (203,),\n",
       " 204: (204,),\n",
       " 205: (205,),\n",
       " 206: (206,),\n",
       " 207: (207,),\n",
       " 208: (208,),\n",
       " 209: (209,),\n",
       " 210: (210,),\n",
       " 211: (211,),\n",
       " 212: (212,),\n",
       " 213: (213,),\n",
       " 214: (214,),\n",
       " 215: (215,),\n",
       " 216: (216,),\n",
       " 217: (217,),\n",
       " 218: (218,),\n",
       " 219: (219,),\n",
       " 220: (220,),\n",
       " 221: (221,),\n",
       " 222: (222,),\n",
       " 223: (223,),\n",
       " 224: (224,),\n",
       " 225: (225,),\n",
       " 226: (226,),\n",
       " 227: (227,),\n",
       " 228: (228,),\n",
       " 229: (229,),\n",
       " 230: (230,),\n",
       " 231: (231,),\n",
       " 232: (232,),\n",
       " 233: (233,),\n",
       " 234: (234,),\n",
       " 235: (235,),\n",
       " 236: (236,),\n",
       " 237: (237,),\n",
       " 238: (238,),\n",
       " 239: (239,),\n",
       " 240: (240,),\n",
       " 241: (241,),\n",
       " 242: (242,),\n",
       " 243: (243,),\n",
       " 244: (244,),\n",
       " 245: (245,),\n",
       " 246: (246,),\n",
       " 247: (247,),\n",
       " 248: (248,),\n",
       " 249: (249,),\n",
       " 250: (250,),\n",
       " 251: (251,),\n",
       " 252: (252,),\n",
       " 253: (253,),\n",
       " 254: (254,),\n",
       " 255: (255,),\n",
       " 256: (32, 116),\n",
       " 257: (32, 97),\n",
       " 258: (104, 101),\n",
       " 259: (105, 110),\n",
       " 260: (256, 104),\n",
       " 261: (114, 101),\n",
       " 262: (32, 111),\n",
       " 263: (32, 44),\n",
       " 264: (101, 114),\n",
       " 265: (32, 115),\n",
       " 266: (111, 114),\n",
       " 267: (32, 105),\n",
       " 268: (97, 116),\n",
       " 269: (32, 46),\n",
       " 270: (110, 100),\n",
       " 271: (32, 119),\n",
       " 272: (111, 110),\n",
       " 273: (32, 99),\n",
       " 274: (32, 98),\n",
       " 275: (32, 102),\n",
       " 276: (101, 110),\n",
       " 277: (111, 117),\n",
       " 278: (101, 115),\n",
       " 279: (257, 110),\n",
       " 280: (97, 110),\n",
       " 281: (262, 102),\n",
       " 282: (32, 112),\n",
       " 283: (259, 103),\n",
       " 284: (101, 100),\n",
       " 285: (105, 115),\n",
       " 286: (105, 116),\n",
       " 287: (97, 108),\n",
       " 288: (32, 109),\n",
       " 289: (97, 114),\n",
       " 290: (32, 100),\n",
       " 291: (32, 104),\n",
       " 292: (256, 111),\n",
       " 293: (111, 109),\n",
       " 294: (108, 101),\n",
       " 295: (116, 101),\n",
       " 296: (105, 99),\n",
       " 297: (115, 101),\n",
       " 298: (118, 101),\n",
       " 299: (32, 121),\n",
       " 300: (32, 101),\n",
       " 301: (97, 115),\n",
       " 302: (32, 108),\n",
       " 303: (105, 108),\n",
       " 304: (114, 111),\n",
       " 305: (32, 110),\n",
       " 306: (111, 116),\n",
       " 307: (32, 117),\n",
       " 308: (276, 116),\n",
       " 309: (274, 101),\n",
       " 310: (32, 38),\n",
       " 311: (267, 115),\n",
       " 312: (299, 277),\n",
       " 313: (105, 111),\n",
       " 314: (32, 261),\n",
       " 315: (102, 111),\n",
       " 316: (111, 115),\n",
       " 317: (99, 101),\n",
       " 318: (256, 258),\n",
       " 319: (116, 104),\n",
       " 320: (116, 105),\n",
       " 321: (275, 266),\n",
       " 322: (32, 103),\n",
       " 323: (97, 121),\n",
       " 324: (121, 111),\n",
       " 325: (97, 100),\n",
       " 326: (262, 110),\n",
       " 327: (111, 119),\n",
       " 328: (101, 116),\n",
       " 329: (32, 73),\n",
       " 330: (117, 116),\n",
       " 331: (32, 65),\n",
       " 332: (32, 259),\n",
       " 333: (117, 114),\n",
       " 334: (260, 97),\n",
       " 335: (108, 108),\n",
       " 336: (268, 105),\n",
       " 337: (264, 115),\n",
       " 338: (109, 101),\n",
       " 339: (257, 114),\n",
       " 340: (113, 117),\n",
       " 341: (114, 105),\n",
       " 342: (258, 114),\n",
       " 343: (291, 97),\n",
       " 344: (99, 104),\n",
       " 345: (117, 115),\n",
       " 346: (287, 108),\n",
       " 347: (97, 109),\n",
       " 348: (100, 101),\n",
       " 349: (105, 114),\n",
       " 350: (111, 108),\n",
       " 351: (336, 272),\n",
       " 352: (32, 80),\n",
       " 353: (286, 104),\n",
       " 354: (32, 83),\n",
       " 355: (267, 116),\n",
       " 356: (112, 111),\n",
       " 357: (268, 101),\n",
       " 358: (32, 67),\n",
       " 359: (265, 104),\n",
       " 360: (273, 293),\n",
       " 361: (32, 64),\n",
       " 362: (271, 104),\n",
       " 363: (45, 64),\n",
       " 364: (97, 112),\n",
       " 365: (361, 45),\n",
       " 366: (112, 101),\n",
       " 367: (116, 111),\n",
       " 368: (110, 116),\n",
       " 369: (257, 115),\n",
       " 370: (32, 118),\n",
       " 371: (97, 99),\n",
       " 372: (266, 100),\n",
       " 373: (104, 105),\n",
       " 374: (305, 111),\n",
       " 375: (103, 101),\n",
       " 376: (320, 272),\n",
       " 377: (105, 103),\n",
       " 378: (107, 101),\n",
       " 379: (105, 118),\n",
       " 380: (257, 108),\n",
       " 381: (271, 286),\n",
       " 382: (117, 111),\n",
       " 383: (340, 306),\n",
       " 384: (32, 77),\n",
       " 385: (105, 100),\n",
       " 386: (117, 110),\n",
       " 387: (97, 118),\n",
       " 388: (105, 101),\n",
       " 389: (261, 115),\n",
       " 390: (285, 116),\n",
       " 391: (98, 108),\n",
       " 392: (290, 101),\n",
       " 393: (32, 71),\n",
       " 394: (271, 101),\n",
       " 395: (262, 114),\n",
       " 396: (108, 100),\n",
       " 397: (105, 109),\n",
       " 398: (108, 121),\n",
       " 399: (114, 97),\n",
       " 400: (116, 97),\n",
       " 401: (101, 101),\n",
       " 402: (111, 100),\n",
       " 403: (109, 97),\n",
       " 404: (260, 105),\n",
       " 405: (260, 105),\n",
       " 406: (278, 115),\n",
       " 407: (303, 108),\n",
       " 408: (307, 115),\n",
       " 409: (282, 304),\n",
       " 410: (32, 50),\n",
       " 411: (289, 116),\n",
       " 412: (32, 59),\n",
       " 413: (32, 76),\n",
       " 414: (275, 114),\n",
       " 415: (99, 97),\n",
       " 416: (112, 108),\n",
       " 417: (273, 272),\n",
       " 418: (273, 97),\n",
       " 419: (101, 97),\n",
       " 420: (364, 316),\n",
       " 421: (414, 293),\n",
       " 422: (111, 111),\n",
       " 423: (32, 40),\n",
       " 424: (117, 108),\n",
       " 425: (48, 48),\n",
       " 426: (32, 69),\n",
       " 427: (99, 116),\n",
       " 428: (105, 97),\n",
       " 429: (115, 116),\n",
       " 430: (32, 41),\n",
       " 431: (32, 49),\n",
       " 432: (312, 114),\n",
       " 433: (32, 68),\n",
       " 434: (97, 105),\n",
       " 435: (111, 112),\n",
       " 436: (115, 111),\n",
       " 437: (274, 121),\n",
       " 438: (102, 101),\n",
       " 439: (305, 101),\n",
       " 440: (265, 101),\n",
       " 441: (296, 104),\n",
       " 442: (105, 102),\n",
       " 443: (299, 101),\n",
       " 444: (32, 87),\n",
       " 445: (110, 101),\n",
       " 446: (266, 116),\n",
       " 447: (278, 116),\n",
       " 448: (300, 118),\n",
       " 449: (318, 121),\n",
       " 450: (32, 66),\n",
       " 451: (271, 266),\n",
       " 452: (97, 103),\n",
       " 453: (32, 79),\n",
       " 454: (102, 116),\n",
       " 455: (104, 287),\n",
       " 456: (362, 296),\n",
       " 457: (265, 116),\n",
       " 458: (271, 303),\n",
       " 459: (32, 70),\n",
       " 460: (108, 111),\n",
       " 461: (32, 72),\n",
       " 462: (32, 258),\n",
       " 463: (104, 116),\n",
       " 464: (117, 109),\n",
       " 465: (268, 101),\n",
       " 466: (343, 298),\n",
       " 467: (359, 346),\n",
       " 468: (264, 121),\n",
       " 469: (277, 116),\n",
       " 470: (288, 97),\n",
       " 471: (307, 112),\n",
       " 472: (108, 97),\n",
       " 473: (286, 121),\n",
       " 474: (265, 117),\n",
       " 475: (273, 104),\n",
       " 476: (389, 115),\n",
       " 477: (307, 110),\n",
       " 478: (310, 35),\n",
       " 479: (259, 101),\n",
       " 480: (46, 46),\n",
       " 481: (261, 101),\n",
       " 482: (277, 108),\n",
       " 483: (257, 99),\n",
       " 484: (300, 120),\n",
       " 485: (331, 270),\n",
       " 486: (32, 84),\n",
       " 487: (49, 50),\n",
       " 488: (262, 117),\n",
       " 489: (262, 117),\n",
       " 490: (269, 46),\n",
       " 491: (261, 110),\n",
       " 492: (301, 116),\n",
       " 493: (360, 112),\n",
       " 494: (100, 259),\n",
       " 495: (117, 99),\n",
       " 496: (271, 97),\n",
       " 497: (280, 116),\n",
       " 498: (302, 97),\n",
       " 499: (97, 98)}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9622ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_and_merges_from_tokens(\n",
    "    tokens: dict[int, tuple[int, ...]],\n",
    "    created_ids: list[int] | None = None,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    tokens:\n",
    "        - base token: tokens[id] = (byte,)      where byte is 0..255\n",
    "        - merged token: tokens[id] = (l_id, r_id)\n",
    "\n",
    "    created_ids:\n",
    "        optional list of merged token ids in creation order.\n",
    "        If None, infer by sorting ids with len(tokens[id])==2.\n",
    "        (This is correct iff merged token ids are assigned monotonically increasing.)\n",
    "    \"\"\"\n",
    "    vocab: dict[int, bytes] = {}\n",
    "    visiting: set[int] = set()\n",
    "\n",
    "    def decode(tid: int) -> bytes:\n",
    "        if tid in vocab:\n",
    "            return vocab[tid]\n",
    "        if tid in visiting:\n",
    "            raise ValueError(f\"Cycle detected at token {tid}\")\n",
    "        if tid not in tokens:\n",
    "            raise KeyError(f\"Token {tid} not found in tokens table\")\n",
    "\n",
    "        visiting.add(tid)\n",
    "        spec = tokens[tid]\n",
    "\n",
    "        if len(spec) == 1:\n",
    "            b = spec[0]\n",
    "            if not (0 <= b <= 255):\n",
    "                raise ValueError(f\"Base token {tid} has invalid byte {b}\")\n",
    "            out = bytes([b])\n",
    "        elif len(spec) == 2:\n",
    "            l_id, r_id = spec\n",
    "            out = decode(l_id) + decode(r_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Token {tid} has invalid arity {len(spec)}\")\n",
    "\n",
    "        visiting.remove(tid)\n",
    "        vocab[tid] = out\n",
    "        return out\n",
    "\n",
    "    # infer creation order of merges if not provided\n",
    "    if created_ids is None:\n",
    "        created_ids = sorted([tid for tid, spec in tokens.items() if len(spec) == 2])\n",
    "\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    for tid in created_ids:\n",
    "        l_id, r_id = tokens[tid]  # guaranteed len==2\n",
    "        lb, rb = decode(l_id), decode(r_id)\n",
    "        merges.append((lb, rb))\n",
    "        decode(tid)  # ensure vocab for this merged token is filled\n",
    "\n",
    "    # ensure vocab for all tokens exists (optional, but usually desired)\n",
    "    for tid in tokens.keys():\n",
    "        decode(tid)\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "79a398e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v,m = build_vocab_and_merges_from_tokens(tokens)\n",
    "tk_id = merge_ops + 256\n",
    "for tk in special_tokens:\n",
    "    v[tk_id] = tk.encode(\"utf-8\")\n",
    "    tk_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c77e7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns a dict mapping byte values (0..255) -> unicode chars,\n",
    "    using the GPT-2 reversible byte encoder.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) \\\n",
    "       + list(range(ord(\"¡\"), ord(\"¬\") + 1)) \\\n",
    "       + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(256):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(256 + n)\n",
    "            n += 1\n",
    "    cs = [chr(c) for c in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "BYTE2UNI = bytes_to_unicode()\n",
    "UNI2BYTE = {v: k for k, v in BYTE2UNI.items()}  # 反向映射（可选）\n",
    "\n",
    "def bytes_to_printable_str(b: bytes) -> str:\n",
    "    return \"\".join(BYTE2UNI[x] for x in b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "af08cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_merges_hf_like(merges: list[tuple[bytes, bytes]], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"#version: 0.2\\n\")\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{bytes_to_printable_str(a)} {bytes_to_printable_str(b)}\\n\")\n",
    "write_merges_hf_like(m,\"/root/workspace/cs336/assignment1/mymerge.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98044e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
