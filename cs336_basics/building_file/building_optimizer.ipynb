{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ff7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from cs336_basics.myModule import toy_Transformer_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b488be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tk_embd.embd torch.Size([10, 20]) True\n",
      "blocks.0.norm1.gain torch.Size([20]) True\n",
      "blocks.0.atte.c_attention.weight torch.Size([60, 20]) True\n",
      "blocks.0.atte.proj.weight torch.Size([20, 20]) True\n",
      "blocks.0.norm2.gain torch.Size([20]) True\n",
      "blocks.0.ff.W1 torch.Size([40, 20]) True\n",
      "blocks.0.ff.W2 torch.Size([20, 40]) True\n",
      "blocks.0.ff.W3 torch.Size([40, 20]) True\n",
      "blocks.1.norm1.gain torch.Size([20]) True\n",
      "blocks.1.atte.c_attention.weight torch.Size([60, 20]) True\n",
      "blocks.1.atte.proj.weight torch.Size([20, 20]) True\n",
      "blocks.1.norm2.gain torch.Size([20]) True\n",
      "blocks.1.ff.W1 torch.Size([40, 20]) True\n",
      "blocks.1.ff.W2 torch.Size([20, 40]) True\n",
      "blocks.1.ff.W3 torch.Size([40, 20]) True\n",
      "norm.gain torch.Size([20]) True\n",
      "out_embd.weight torch.Size([10, 20]) True\n"
     ]
    }
   ],
   "source": [
    "test_ml = toy_Transformer_lm(10,10,20,2,4,40,0.00001)\n",
    "for name, p in test_ml.named_parameters():\n",
    "    print(name, p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83178617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class toy_AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params,betas=(0.9,0.999),weight_decay = 0,eps = 1e-8,lr = 1e-3) -> None:\n",
    "        defaults = dict(lr=lr,beta1 = betas[0],beta2 = betas[1], decay = weight_decay, eps = eps)\n",
    "        super().__init__(params, defaults)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure = None):\n",
    "        loss = closure() if closure is not None else None\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta1 = group[\"beta1\"]\n",
    "            beta2 = group[\"beta2\"]\n",
    "            decay = group[\"decay\"]\n",
    "            eps = group[\"eps\"]\n",
    "            \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) + 1 # Get iteration number from the state, or initial value.\n",
    "                m = state.get(\"m\", torch.zeros_like(p))\n",
    "                v = state.get(\"v\", torch.zeros_like(p))\n",
    "                grad = p.grad\n",
    "                \n",
    "                m_new = beta1 * m + (1 - beta1)* grad\n",
    "                v_new = beta2 * v + (1 - beta2)* grad**2\n",
    "                lr_now = lr * ((1 - beta2**t)**(0.5)) / (1 - beta1**t)\n",
    "                p -= lr_now * m_new / (v_new**0.5 + eps) + lr*decay*p\n",
    "                \n",
    "                state[\"t\"] = t\n",
    "                state[\"m\"] = m_new\n",
    "                state[\"v\"] = v_new                \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725707b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def toy_grad_clip(parameters,max_l2_norm):\n",
    "    norm_all = torch.zeros(1)\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            norm_all += p.grad.norm().square()\n",
    "    norm_all = norm_all.sqrt()\n",
    "    if norm_all > max_l2_norm:\n",
    "        scale = max_l2_norm / norm_all\n",
    "        for p in parameters:\n",
    "            if p.grad is not None:\n",
    "                p.grad.mul_(scale)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee14afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(91.0000, grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Parameter(torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float32))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
